defaults:
  - base_config
  - _self_
project: cifar10-DP
general:
  log_wandb: False
  parallel: False
  eval_train: False
  use_pretrained_model: 
  #save_path: cifar10dp.npz
# checkpoint:
#   logdir: cifar10dpckpts
#   keep_ckpts: 2
#   makedir: True
dataset: 
  task: classification
  name: CIFAR10
  root: ./data
  train_val_split: 0.9
  normalization: True
  fft: False
metrics:
  main: 
    accuracy_score:
  logging:
    f1_score:
      average: macro
loader: 
  num_workers: 16
  prefetch_factor: 32
  collate_fn: numpy
augmentations:
  consecutive_augmentations:
    multiplicity: 6
    consecutive_augmentations:
      random_img_shift:
        max_shift: 4
      random_vertical_flips:
        flip_prob: 0.5
model:
  name: wide_resnet
  in_channels: 3
  num_classes: 10
  conv: convws_nw
  activation: mish
  normalization: gn
  pooling: avgpool
  ensemble: 1
  extra_args:
    depth: 16
    width: 4
    scale_norm: True
optim:
  name: nadam
loss: 
  type: cse
  reduction: mean
ema:
  use_ema: True
  decay: 0.001
hyperparams:
  epochs: 75
  batch_size: 256
  batch_size_test: 512
  lr: 2e-3
  # momentum: 0.9
scheduler:
  type: reduceonplateau
  normalize_lr: False
  patience: 20
  min_delta: 0.01
  factor: 0.5
# earlystopping:
#   mode: maximize
#   patience: 100
#   min_delta: 0.01
DP:
  epsilon: 8
  # sigma: 3
  max_per_sample_grad_norm: 1.0
  delta: 1e-5
  norm_acc: False
  grad_acc_steps: 16
  rsqrt_noise_adapt: False
  glrt_assumption: False
  bam: False
  r: 0.1
  alpha: 0.31
