import math
import os
import sys
from argparse import ArgumentParser
from copy import deepcopy
from datetime import datetime
from pathlib import Path
from warnings import warn

import cv2
import numpy as np
import pandas as pd
import torch
from omegaconf import open_dict
from opacus.accountants import create_accountant
from opacus.accountants.utils import get_noise_multiplier
from tqdm import tqdm, trange

sys.path.insert(0, str(Path.cwd()))
os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
# os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"] = "platform"
# if train_config.general.cpu:
# os.environ["CUDA_VISIBLE_DEVICES"] = ""


from breaching import breaching
from breachingobjaxutils.initialize import make_configs


from jax import numpy as jn

from breachingobjaxutils.objaxbasedfunctions import (
    get_aug_normalization,
    get_transform_normalization,
    make_make_fns,
)
from dptraining.datasets import make_loader_from_config
from dptraining.privacy import analyse_epsilon, setup_privacy
from dptraining.utils.training_utils import fix_seeds

parser = ArgumentParser()
parser.add_argument("-cn", "--config", required=True, help="Path to training config.")
parser.add_argument(
    "-b", "--num_bins", default=None, help="Number of reconstruction images", type=int
)
parser.add_argument(
    "-nb", "--N_batches", default=5, help="How many batches to reconstruct", type=int
)
parser.add_argument(
    "-eb",
    "--eps_range_begin",
    type=int,
    default=9,
    help="Exponential factor for first epsilon",
)
parser.add_argument(
    "-ee",
    "--eps_range_end",
    type=int,
    default=18,
    help="Exponential factor for last epsilon",
)
parser.add_argument(
    "-es",
    "--eps_range_steps",
    type=int,
    default=3,
    help="Step size between eps exp factors",
)
parser.add_argument(
    "--privacy_for_one_step",
    action="store_true",
    help="Calculate noise as if only one step was performed",
)
args = parser.parse_args()

cfg, config = make_configs(
    ["attack=imprint", "case/server=malicious-model-cah"],
    args.config,
)


fix_seeds(config)


class ProxySet(torch.utils.data.Dataset):
    def __init__(self, batches: list) -> None:
        super().__init__()
        self.batches: list = batches

    def __len__(self):
        return len(self.batches)

    def __getitem__(self, i: int):
        return self.batches[i]


def save_imgs_to_path(imgs, batch_p_recon):
    counter = 0
    if not batch_p_recon.is_dir():
        batch_p_recon.mkdir(parents=True)
    for img in imgs:
        if len(img.shape) > 3:
            np.save(
                str(batch_p_recon / f"{counter}.npy"),
                img,
                allow_pickle=True,
                fix_imports=False,
            )
        else:
            cv2.imwrite(str(batch_p_recon / f"{counter}.png"), img.astype(np.uint8))
        counter += 1


def convert_to_cv2(recon_imgs):
    recon_imgs = recon_imgs.transpose(0, 2, 3, 1)
    recon_imgs = recon_imgs - recon_imgs.min(
        axis=tuple(range(1, len(recon_imgs.shape))), keepdims=True
    )
    normaliser = recon_imgs.max(
        axis=tuple(range(1, len(recon_imgs.shape))), keepdims=True
    )
    normaliser = np.where(np.abs(normaliser) < 1e-9, 1.0, normaliser)
    recon_imgs /= normaliser
    recon_imgs *= 255.0
    if recon_imgs.shape[-1] == 3:  # convert RBG to BRG
        recon_imgs = recon_imgs[..., [2, 1, 0]]
    return recon_imgs


def normalise_array_to_zero_one(x, axis):
    x = x - x.min(axis=axis, keepdims=True)
    denom = x.max(axis=axis, keepdims=True)
    denom = np.where(denom < 1e-9, 1.0, denom)
    return x / denom


# device = torch.device("cpu")
torch.backends.cudnn.benchmark = cfg.case.impl.benchmark

eps_values = [
    10**i
    for i in range(args.eps_range_begin, args.eps_range_end + 1, args.eps_range_steps)
] + ["Non-private"]
# eps_values = [1, 8, 20, 1e9, 1e12, "Non-private"]
# eps_values = ["Non-private"] + [1e15]

recon_data_per_eps = []
save_directory = Path(
    f"reconstructions/{str(config.dataset.name).split('.')[-1]}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/"
)

train_loader, _, _ = make_loader_from_config(config)

accountant = create_accountant(config.DP.mechanism)

batches = []
for i, batch in enumerate(train_loader):
    batches.append(batch)
    if i > args.N_batches * config.hyperparams.grad_acc_steps + 1:
        break

proxy_dataset = ProxySet(batches)
proxy_loader = torch.utils.data.DataLoader(
    proxy_dataset, batch_size=1, collate_fn=lambda _: _[0]
)
example_batch, example_label = proxy_dataset[0]
data_dimensions = sum([s > 3 for s in example_batch.shape[1:]])

aug_fn = None
stats = get_transform_normalization(config)
if stats is not None:
    dm, ds = stats
else:
    stats = get_aug_normalization(config)
    if stats is not None:
        dm, ds, aug_fn = stats
    elif (
        config.dataset.nifti_seg_options is not None
        and config.dataset.nifti_seg_options.data_stats is not None
    ):
        dm, ds = (
            np.array([config.dataset.nifti_seg_options.data_stats.mean]),
            np.array([config.dataset.nifti_seg_options.data_stats.std]),
        )
    else:
        warn("Did not find stats. Will continue assuming unnormalized input.")
        dm, ds = np.array([0.0]), np.array([1.0])
if (
    config.dataset.nifti_seg_options is not None
    and config.dataset.nifti_seg_options.ct_window is not None
):
    recon_clip_threshold = (
        config.dataset.nifti_seg_options.ct_window.low,
        config.dataset.nifti_seg_options.ct_window.high,
    )
else:
    recon_clip_threshold = (0.0, 1.0)

new_shape = [1] * len(example_batch.shape)
new_shape[1] = ds.shape[0]
dm, ds = dm.reshape(*new_shape), ds.reshape(*new_shape)
stats = (dm, ds)
sigmas = {}

for budget_iteration, eps in tqdm(
    enumerate(eps_values), total=len(eps_values), desc="Privacy levels", leave=False
):
    eps_path = save_directory / (eps if isinstance(eps, str) else f"eps={eps:.0E}")
    train_config = deepcopy(config)
    if isinstance(eps, (float, int)):
        train_config.DP.epsilon = eps
        train_config.DP.eps_tol = eps * 10e-6
    else:
        train_config.DP = None

    make_model, make_loss_gv_from_model = make_make_fns(train_config)

    if train_config.DP:
        if args.privacy_for_one_step:
            effective_batch_size = (
                config.hyperparams.grad_acc_steps * config.hyperparams.batch_size
            )
            batch_expansion_factor = config.hyperparams.grad_acc_steps
            sampling_rate: float = effective_batch_size / len(train_loader.dataset)
            delta = config.DP.delta
            steps = 1
            opt_func = lambda noise: analyse_epsilon(
                accountant, steps, noise, sampling_rate, delta
            )
            sigma = get_noise_multiplier(
                target_epsilon=train_config.DP.epsilon,
                target_delta=delta,
                sample_rate=sampling_rate,
                steps=steps,
                accountant=config.DP.mechanism,
                epsilon_tolerance=train_config.DP.epsilon / 1e6,
            )
            total_noise = sigma * config.DP.max_per_sample_grad_norm
        else:
            (
                grad_acc,
                accountant,
                sampling_rate,
                delta,
                sigma,
                total_noise,
                batch_expansion_factor,
                effective_batch_size,
            ) = setup_privacy(train_config, train_loader)

            steps = (
                len(train_loader) // batch_expansion_factor
            ) * train_config.hyperparams.epochs
        with open_dict(cfg):
            cfg.case.user.total_noise = total_noise
        print(
            f"actual epsilon: {analyse_epsilon(accountant,steps,sigma,sampling_rate,delta, config.DP.alphas):.2E} with noise multiplier {sigma}"
        )
        sigmas[eps_path.stem] = sigma
    if (
        train_config.hyperparams.batch_size * train_config.hyperparams.grad_acc_steps
        == 1
    ):
        cfg.case.server.model_modification.type = "ImprintBlock"
        cfg.case.user.num_data_points = 1
        cfg.case.server.model_modification.num_bins = 10
        cfg.case.server.model_modification.position = None
        cfg.case.server.model_modification.connection = "mean"  # "linear_and_add"
        with open_dict(cfg):
            cfg.case.server.model_modification.linfunc = "avg"
            if "sigma" in cfg.case.server.model_modification:
                del cfg.case.server.model_modification.sigma
                del cfg.case.server.model_modification.mu
                del cfg.case.server.model_modification.scale_factor
        # cfg.case.server.model_modification.sigma = 0.5  # float(0.5 * np.mean(ds))
        # cfg.case.server.model_modification.mu = 0.0  # float(
        cfg.case.data.normalize = True
        cfg.case.data.mean = None
        cfg.case.data.std = None
        cfg.case.data.shape = example_batch.shape[1:]
        #     -np.mean(dm) * math.sqrt(np.prod(example_batch.shape[1:])) * 0.5
        # )
        # cfg.case.server.model_modification.scale_factor = 1.01  # -0.9990
        cfg.attack.breach_reduction = None  # Will be done manually

    else:
        cfg.case.user.num_data_points = config.hyperparams.batch_size
        cfg.case.server.model_modification.type = "CuriousAbandonHonesty"
        cfg.case.server.model_modification.num_bins = (
            args.num_bins
            if args.num_bins
            else config.hyperparams.batch_size * config.hyperparams.grad_acc_steps * 2
        )

        cfg.case.server.model_modification.position = None
        cfg.case.server.model_modification.connection = "addition"
        cfg.case.data.normalize = True
        cfg.case.data.mean = None
        cfg.case.data.std = None
        cfg.case.data.shape = example_batch.shape[1:]
        cfg.case.server.model_modification.sigma = float(0.5 * np.mean(ds))
        cfg.case.server.model_modification.mu = float(
            -np.mean(dm) * math.sqrt(np.prod(example_batch.shape[1:])) * 0.5
        )
        cfg.case.server.model_modification.scale_factor = -0.9990
        cfg.attack.breach_reduction = None  # Will be done manually

    user, server, model_fn = breaching.cases.construct_case(
        train_config,
        cfg.case,
        make_model,
        dataloader=(proxy_loader, aug_fn),
        make_loss_grad=make_loss_gv_from_model,
    )
    attacker = breaching.attacks.prepare_attack(
        model_fn, make_loss_gv_from_model, cfg.attack, stats
    )
    server_payload = server.distribute_payload()
    min_dist_total = []
    recon_counter = 0
    gt_counter = 0
    stats_df = None
    for batch in tqdm(
        range(args.N_batches),
        total=args.N_batches,
        desc="reconstructing inputs",
        leave=False,
    ):
        shared_data, true_user_data = user.compute_local_updates(server_payload)

        reconstructed_user_data, recon_stats = attacker.reconstruct(
            [server_payload],
            [shared_data],
            server.secrets,
            dryrun=cfg.dryrun,
            rescale=config.dataset.nifti_seg_options is None,
        )
        true_user_data = {k: np.array(v) for k, v in true_user_data.items()}
        del shared_data
        reconstructed_user_data["data"] = np.minimum(
            np.maximum(
                np.array(reconstructed_user_data["data"]) * ds + dm,
                recon_clip_threshold[0],
            ),
            recon_clip_threshold[1],
        )
        true_user_data["data"] = np.minimum(
            np.maximum(
                np.array(true_user_data["data"]) * ds + dm, recon_clip_threshold[0]
            ),
            recon_clip_threshold[1],
        )
        axis = tuple(range(1, len(true_user_data["data"].shape)))
        reconstructed_user_data["data"] = normalise_array_to_zero_one(
            reconstructed_user_data["data"], axis
        )
        true_user_data["data"] = normalise_array_to_zero_one(
            true_user_data["data"], axis
        )
        batch_p_recon = eps_path / f"{batch}"
        if data_dimensions > 2:
            recon_imgs = reconstructed_user_data["data"]
            # convert_to_cv2(
            #     reconstructed_user_data["data"][
            #         ..., reconstructed_user_data["data"].shape[-1] // 2
            #     ]
            # )
            if budget_iteration == 0:
                gt_imgs = true_user_data["data"]
                # convert_to_cv2(
                # true_user_data["data"][..., true_user_data["data"].shape[-1] // 2]
                # )
        else:
            recon_imgs = convert_to_cv2(reconstructed_user_data["data"])
            if budget_iteration == 0:
                gt_imgs = convert_to_cv2(true_user_data["data"])
        save_imgs_to_path(recon_imgs, batch_p_recon)
        if budget_iteration == 0:
            batch_p_gt = save_directory / "gt" / f"{batch}"
            save_imgs_to_path(gt_imgs, batch_p_gt)
if len(sigmas) >= 1:
    pd.DataFrame(data={"noise_multipliers": sigmas}).to_csv(
        save_directory / "noise_multipliers.csv"
    )
