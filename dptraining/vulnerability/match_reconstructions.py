from argparse import ArgumentParser
from pathlib import Path
from typing import Callable

import numpy as np
import pandas as pd
import piq
import torch
from torch.utils.data import DataLoader, Dataset
from torchvision.datasets.folder import default_loader
from torchvision.transforms import ToTensor
from tqdm import tqdm, trange
from warnings import warn, catch_warnings, simplefilter
from datetime import datetime


def calc_perc_dist_direct(features_gt, features_rec):
    if not torch.is_floating_point(features_gt):
        features_gt = features_gt.to(torch.float16, device=features_gt.device)
        features_rec = features_rec.to(torch.float16, device=features_rec.device)
    if features_gt.dtype == torch.float32:
        with catch_warnings(), torch.inference_mode():
            simplefilter("ignore")
            return torch.nn.functional.mse_loss(
                features_gt.view(features_gt.shape[0], 1, *features_gt.shape[1:]),
                features_rec,
                reduction="none",
            )[..., 0]
    
    return torch.pow(features_gt.view(features_gt.shape[0], 1, *features_gt.shape[1:]) - features_rec, 2).sum(dim=2).cpu()


def mean_diff_torch_alongX(a1: torch.Tensor, a2: torch.Tensor):
    X, N = a1.shape
    X1, M = a2.shape
    assert X == X1
    out = torch.zeros((N, M), device=a1.device)
    for l, r in tqdm(zip(a1, a2), total=X, leave=False):
        out += torch.square(l[:, None] - r)
    out /= X
    return out


def mean_diff_torch_alongNX(
    a1: torch.Tensor,
    a2: torch.Tensor,
    patchsize: int = 50,
    iterative: bool = False,
    device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
):
    # device = a1.device
    N, X = a1.shape
    M, X1 = a2.shape
    assert X == X1
    out = torch.zeros((N, M), dtype=torch.float32)
    a1 = a1.T.clone()#.to(dtype=torch.float16)
    if iterative:
        a2 = a2.T.clone().to(device)#, dtype=torch.float16)
    else:
        # a1 = a1
        a2 = a2.clone().to(device)#, dtype=torch.float16)
    N_patches = N // patchsize + (N % patchsize > 0) * 1
    for i in trange(N_patches, leave=False, desc="Distance patches"):
        if iterative:
            patch = a1[:, i * patchsize : (i + 1) * patchsize].to(device)  # .clone()
            out[i * patchsize : (i + 1) * patchsize] = mean_diff_torch_alongX(
                patch, a2
            ).cpu()
        else:
            torch.cuda.empty_cache()
            patch = a1[:, i * patchsize : (i + 1) * patchsize].T.clone().to(device)
            out[i * patchsize : (i + 1) * patchsize] = calc_perc_dist_direct(
                patch, a2
            ).cpu()
        del patch
    return out


def calc_mse_diff(
    args,
    features_gt,
    features_rec,
    device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
):
    N, M, X = features_gt.shape[0], features_rec.shape[0], features_gt.shape[1]
    imgs_per_iter: int = int(args.gpu_memory_limit / np.prod(features_rec.shape))
    if (
        N * M * X < args.gpu_memory_limit
    ):  # this is just an empirical threshold and strongly depends on the hardware
        perc_dist = calc_perc_dist_direct(
            features_gt.to(device), features_rec.to(device)
        ).T
    elif imgs_per_iter < 1:
        perc_dist = mean_diff_torch_alongNX(
            features_rec,
            features_gt,
            500,
            iterative=True,
        )
    else:  # memory optimised version
        perc_dist = mean_diff_torch_alongNX(
            features_rec,
            features_gt,
            max(imgs_per_iter,1),
            iterative=False,
        )

    min_dist, closest_recon_to_gt = torch.min(perc_dist, dim=0)[0], torch.argmin(
        perc_dist, dim=0
    )
    return min_dist, closest_recon_to_gt


class FolderDataset(Dataset):
    def __init__(
        self,
        folder_path: Path,
        loader_fn: Callable = default_loader,
        transform: Callable = torch.nn.Identity(),
    ) -> None:
        super().__init__()
        self.files = [f for f in folder_path.iterdir() if f.is_file()]
        self.files = sorted(self.files, key=lambda x: int(x.stem))
        self.loader_fn = loader_fn
        self.transform = transform

    def __len__(self) -> int:
        return len(self.files)

    def __getitem__(self, index) -> torch.Tensor:
        return self.transform(self.loader_fn(self.files[index]))


def collate_fn(batch_list: list[torch.Tensor]) -> dict[str, torch.Tensor]:
    return {"images": torch.stack(batch_list, dim=0)}


def calc_lpips_features(
    args,
    lpips_metric,
    lpips_feat_shape,
    device,
    dataloader,
    # min_input=0.0,
    # max_input=1.0,
):
    gt_lpips_features = torch.zeros(
        (len(dataloader.dataset), lpips_feat_shape), dtype=torch.float16
    )
    for i, x in tqdm(
        enumerate(dataloader),
        total=len(dataloader),
        desc="Calc lpips feats",
        leave=False,
    ):
        feats = concat_lpips_feats(lpips_metric, device, x)
        # if feats.min().item() < min_input:
        #     warn(f"Feature smaller than {min_input} encountered")
        # if feats.max().item() > max_input:
        #     warn(f"Feature larger than {max_input} encountered")
        # feats = (feats + min_input) * 256.0 / max_input
        # feats = torch.clamp(feats, min=0, max=256)
        # feats = feats.to(dtype=torch.uint8)
        gt_lpips_features[i * args.batch_size : (i + 1) * args.batch_size] = feats

    return gt_lpips_features


def concat_lpips_feats(lpips_metric, device, x):
    return torch.concatenate(
        [
            x_i.cpu().flatten(1)
            for x_i in lpips_metric.get_features(x["images"].to(device))
        ],
        dim=1,
    )


if __name__ == "__main__":
    id_str = f"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
    parser = ArgumentParser()
    parser.add_argument(
        "--recon_folder",
        type=Path,
        default=None,
        required=True,
        help="Folder with reconstructions",
    )
    parser.add_argument(
        "--gpu_memory_limit",
        type=float,
        default=1e10,
        help="Threshold to limit the maximum gpu capacity. If lower will reduce memory consumption but take longer.",
    )
    parser.add_argument(
        "--classical_metrics",
        action="store_true",
        help="Also calculate SSIM and MSE (slow!)",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1024,
        help="Batch size for calculating features",
    )
    parser.add_argument(
        "--num_workers", type=int, default=2, help="Number of workers for dataloader"
    )
    parser.add_argument(
        "--prefetch_factor", type=int, default=4, help="Prefetch factor for dataloader"
    )
    args = parser.parse_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16

    read_directory = Path(args.recon_folder)
    privacy_level_dirs = {
        d.stem.replace("eps=", ""): d for d in read_directory.iterdir() if d.is_dir()
    }
    privacy_level_dirs = dict(sorted(privacy_level_dirs.items()))
    gt_folder = privacy_level_dirs["gt"]
    assert gt_folder.is_dir()
    del privacy_level_dirs["gt"]
    batches = [int(d.stem) for d in gt_folder.iterdir() if d.is_dir()]
    batches = max(batches)

    fid_metric = piq.FID()
    fid_extractor = piq.feature_extractors.InceptionV3()
    # lpips_metric = piq.ContentLoss(
    #     layers=('relu2_2', 'relu3_3', 'relu4_3'),#'relu2_2', ),
    #     weights=(1.0,1.0,1.0),
    #     normalize_features=True,
    # )  # full lpips exceeds all memory budgets we have
    lpips_metric = piq.LPIPS()
    lpips_metric.to(device)

    example_lpips_feat = concat_lpips_feats(
        lpips_metric,
        device,
        {
            "images": FolderDataset(
                gt_folder / "0",
                transform=ToTensor(),
            )[
                0
            ].unsqueeze(0)
        },
    )
    lpips_feat_shape = example_lpips_feat.shape[1]

    results_dict = {
        "batch_id": [],
        "img_id": [],
        "gt_image": [],
        **{
            f"closest_recon_fid_{privacy_level}": []
            for privacy_level in privacy_level_dirs.keys()
        },
        **{
            f"recon_distance_fid_{privacy_level}": []
            for privacy_level in privacy_level_dirs.keys()
        },
        **{
            f"closest_recon_lpips_{privacy_level}": []
            for privacy_level in privacy_level_dirs.keys()
        },
        **{
            f"recon_distance_lpips_{privacy_level}": []
            for privacy_level in privacy_level_dirs.keys()
        },
    }
    if args.classical_metrics:
        results_dict.update(
            {
                **{
                    f"closest_recon_ssim_{privacy_level}": []
                    for privacy_level in privacy_level_dirs.keys()
                },
                **{
                    f"ssim_distance_{privacy_level}": []
                    for privacy_level in privacy_level_dirs.keys()
                },
                **{
                    f"closest_recon_mse_{privacy_level}": []
                    for privacy_level in privacy_level_dirs.keys()
                },
                **{
                    f"recon_distance_mse_{privacy_level}": []
                    for privacy_level in privacy_level_dirs.keys()
                },
            }
        )
    for batch_num in trange(batches, desc="Batches", leave=False):
        # gt_data = load_data(gt_folder, batch_num)
        gt_batch_path = gt_folder / str(batch_num)
        gt_dataset = FolderDataset(
            gt_batch_path,
            transform=ToTensor(),
        )
        gt_loader = DataLoader(
            gt_dataset,
            collate_fn=collate_fn,
            num_workers=args.num_workers,
            batch_size=args.batch_size,
            pin_memory=False,
            prefetch_factor=args.prefetch_factor,
            shuffle=False,
        )
        lpips_metric.to(device)
        with torch.inference_mode():  # , torch.autocast(device_type=device, dtype=dtype):
            gt_fid_features = fid_metric.compute_feats(
                gt_loader, feature_extractor=fid_extractor
            )
            # with torch.autocast(device_type=device, dtype=dtype):
            gt_lpips_features = calc_lpips_features(
                    args,
                    lpips_metric,
                    lpips_feat_shape,
                    device,
                    gt_loader,  # , dtype=dtype
                )

        # assert np.isnan(gt_lpips_features).sum() == 0

        batch_dict = {
            "batch_id": [batch_num for _ in range(len(gt_loader.dataset))],
            "img_id": list(range(len(gt_loader.dataset))),
            "gt_image": [
                gt_batch_path / f"{i}.png" for i in range(len(gt_loader.dataset))
            ],
        }

        for privacy_level, recon_folder in tqdm(
            privacy_level_dirs.items(),
            total=len(privacy_level_dirs),
            desc="Privacy Levels",
            leave=False,
        ):
            recon_batch_path = recon_folder / str(batch_num)
            recon_loader = DataLoader(
                FolderDataset(
                    recon_batch_path,
                    transform=ToTensor(),
                ),
                collate_fn=collate_fn,
                num_workers=args.num_workers,
                batch_size=args.batch_size,
                pin_memory=False,
                prefetch_factor=args.prefetch_factor,
                shuffle=False,
            )

            with torch.inference_mode():  # , :
                recon_fid_features = fid_metric.compute_feats(
                    recon_loader,
                    feature_extractor=fid_extractor,
                )
                lpips_metric.to(device)
                # with torch.autocast(device_type=device, dtype=dtype):
                recon_lpips_features = calc_lpips_features(
                        args,
                        lpips_metric,
                        lpips_feat_shape,
                        device,
                        recon_loader,
                        # dtype=dtype,
                    )
                # assert np.isnan(recon_lpips_features).sum() == 0

                lpips_metric.to("cpu")
                torch.cuda.empty_cache()
                min_dist_fid, closest_recon_to_gt_fid = calc_mse_diff(
                    args, gt_fid_features, recon_fid_features
                )
                torch.cuda.empty_cache()
                min_dist_lpips, closest_recon_to_gt_lpips = calc_mse_diff(
                    args, gt_lpips_features, recon_lpips_features
                )
                torch.cuda.empty_cache()
                if args.classical_metrics:
                    ssims = torch.zeros((len(gt_dataset), len(recon_loader.dataset)))
                    mses = torch.zeros((len(gt_dataset), len(recon_loader.dataset)))
                    for i, gt_img in tqdm(
                        enumerate(gt_dataset),
                        total=len(gt_dataset),
                        desc="calc traditional metrics",
                        leave=False,
                    ):
                        gt_img = gt_img.to(device)
                        for j, batch in tqdm(
                            enumerate(recon_loader),
                            total=len(recon_loader),
                            leave=False,
                        ):
                            batch = batch["images"].to(device)
                            expanded_img = gt_img.expand(batch.shape)
                            ssims[
                                i, j * args.batch_size : (j + 1) * args.batch_size
                            ] = piq.ssim(
                                x=expanded_img,
                                y=batch,
                                reduction="none",
                            ).cpu()
                            mses[
                                i, j * args.batch_size : (j + 1) * args.batch_size
                            ] = torch.nn.functional.mse_loss(
                                expanded_img, batch, reduction="none"
                            ).mean(
                                dim=list(range(1, len(batch.shape)))
                            )
                    closest_recon_to_gt_ssim = ssims.argmax(dim=1)
                    best_ssim = ssims.max(dim=1)[0]
                    closest_recon_to_gt_mse = mses.argmin(dim=1)
                    min_dist_mse = mses.min(dim=1)[0]

            batch_dict[
                f"closest_recon_fid_{privacy_level}"
            ] = closest_recon_to_gt_fid.flatten().tolist()
            batch_dict[
                f"recon_distance_fid_{privacy_level}"
            ] = min_dist_fid.flatten().tolist()
            batch_dict[
                f"closest_recon_lpips_{privacy_level}"
            ] = closest_recon_to_gt_lpips.flatten().tolist()
            batch_dict[
                f"recon_distance_lpips_{privacy_level}"
            ] = min_dist_lpips.flatten().tolist()
            if args.classical_metrics:
                batch_dict[
                    f"closest_recon_ssim_{privacy_level}"
                ] = closest_recon_to_gt_ssim.flatten().tolist()
                batch_dict[
                    f"ssim_distance_{privacy_level}"
                ] = best_ssim.flatten().tolist()
                batch_dict[
                    f"closest_recon_mse_{privacy_level}"
                ] = closest_recon_to_gt_mse.flatten().tolist()
                batch_dict[
                    f"recon_distance_mse_{privacy_level}"
                ] = min_dist_mse.flatten().tolist()
        for key, value in results_dict.items():
            results_dict[key] += batch_dict[key]

    total_df = pd.DataFrame.from_dict(results_dict)
    total_df.to_csv(str(read_directory / f"perceptual_distances_{id_str}.csv"))
