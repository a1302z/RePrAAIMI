from argparse import ArgumentParser
from pathlib import Path
from typing import Callable
from sys import path

import numpy as np
import pandas as pd
import piq
import torch
from torch.utils.data import DataLoader, Dataset
from torchvision.datasets.folder import default_loader
from torchvision.transforms import ToTensor
from tqdm import tqdm, trange
from warnings import warn, catch_warnings, simplefilter
from datetime import datetime

path.insert(0, str(Path.cwd()))

from dptraining.vulnerability.vectorized_ssim import ssim_fn


AVAILABLE_METRICS = [
    "mse",
    "ssim",
    "fid",
    "lpips",
]


def calc_perc_dist_direct(features_gt, features_rec):
    if not torch.is_floating_point(features_gt):
        features_gt = features_gt.to(torch.float16, device=features_gt.device)
        features_rec = features_rec.to(torch.float16, device=features_rec.device)
    if features_gt.dtype == torch.float32:
        with catch_warnings(), torch.inference_mode():
            simplefilter("ignore")
            return torch.nn.functional.mse_loss(
                features_gt.view(features_gt.shape[0], 1, *features_gt.shape[1:]),
                features_rec,
                reduction="none",
            )[..., 0]

    return (
        torch.pow(
            features_gt.view(features_gt.shape[0], 1, *features_gt.shape[1:])
            - features_rec,
            2,
        )
        .sum(dim=2)
        .cpu()
    )


def mean_diff_torch_alongX(a1: torch.Tensor, a2: torch.Tensor):
    X, N = a1.shape
    X1, M = a2.shape
    assert X == X1
    out = torch.zeros((N, M), device=a1.device)
    for l, r in tqdm(zip(a1, a2), total=X, leave=False):
        out += torch.square(l[:, None] - r)
    out /= X
    return out


def mean_diff_torch_alongNX(
    a1: torch.Tensor,
    a2: torch.Tensor,
    patchsize: int = 50,
    iterative: bool = False,
    device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
):
    # device = a1.device
    N, X = a1.shape
    M, X1 = a2.shape
    assert X == X1
    out = torch.zeros((N, M), dtype=torch.float32)
    a1 = a1.T.clone()  # .to(dtype=torch.float16)
    if iterative:
        a2 = a2.T.clone().to(device)  # , dtype=torch.float16)
    else:
        # a1 = a1
        a2 = a2.clone().to(device)  # , dtype=torch.float16)
    N_patches = N // patchsize + (N % patchsize > 0) * 1
    for i in trange(N_patches, leave=False, desc="Distance patches"):
        if iterative:
            patch = a1[:, i * patchsize : (i + 1) * patchsize].to(device)  # .clone()
            out[i * patchsize : (i + 1) * patchsize] = mean_diff_torch_alongX(
                patch, a2
            ).cpu()
        else:
            torch.cuda.empty_cache()
            patch = a1[:, i * patchsize : (i + 1) * patchsize].T.clone().to(device)
            out[i * patchsize : (i + 1) * patchsize] = calc_perc_dist_direct(
                patch, a2
            ).cpu()
        del patch
    return out


def calc_mse_diff(
    args,
    features_gt,
    features_rec,
    device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    patchsize=1,
):
    N, M, X = features_gt.shape[0], features_rec.shape[0], features_gt.shape[1]
    imgs_per_iter: int = int(args.gpu_memory_limit / np.prod(features_rec.shape))
    if (
        N * M * X < args.gpu_memory_limit
    ):  # this is just an empirical threshold and strongly depends on the hardware
        perc_dist = calc_perc_dist_direct(
            features_gt.to(device), features_rec.to(device)
        ).T
    elif imgs_per_iter < 1:
        perc_dist = mean_diff_torch_alongNX(
            features_rec,
            features_gt,
            patchsize,
            iterative=True,
        )
    else:  # memory optimised version
        perc_dist = mean_diff_torch_alongNX(
            features_rec,
            features_gt,
            patchsize,
            iterative=False,
        )

    min_dist, closest_recon_to_gt = torch.min(perc_dist, dim=0)[0], torch.argmin(
        perc_dist, dim=0
    )
    return min_dist, closest_recon_to_gt, perc_dist


class FolderDataset(Dataset):
    def __init__(
        self,
        folder_path: Path,
        loader_fn: Callable = default_loader,
        transform: Callable = torch.nn.Identity(),
        ignore_img_idcs: list[bool] = [],
    ) -> None:
        super().__init__()
        self.files = [f for f in folder_path.iterdir() if f.is_file()]
        self.files = sorted(self.files, key=lambda x: int(x.stem))
        self.files = np.array(self.files)
        if len(ignore_img_idcs) > 0:
            self.files = self.files[ignore_img_idcs]
        self.loader_fn = loader_fn
        self.transform = transform

    def __len__(self) -> int:
        return len(self.files)

    def __getitem__(self, index) -> torch.Tensor:
        return self.transform(self.loader_fn(self.files[index]))


def collate_fn(batch_list: list[torch.Tensor]) -> dict[str, torch.Tensor]:
    return {"images": torch.stack(batch_list, dim=0)}


def calc_lpips_features(
    args,
    lpips_metric,
    lpips_feat_shape,
    device,
    dataloader,
    # min_input=0.0,
    # max_input=1.0,
):
    gt_lpips_features = torch.zeros(
        (len(dataloader.dataset), lpips_feat_shape), dtype=torch.float16
    )
    for i, x in tqdm(
        enumerate(dataloader),
        total=len(dataloader),
        desc="Calc lpips feats",
        leave=False,
    ):
        feats = concat_lpips_feats(lpips_metric, device, x)
        # if feats.min().item() < min_input:
        #     warn(f"Feature smaller than {min_input} encountered")
        # if feats.max().item() > max_input:
        #     warn(f"Feature larger than {max_input} encountered")
        # feats = (feats + min_input) * 256.0 / max_input
        # feats = torch.clamp(feats, min=0, max=256)
        # feats = feats.to(dtype=torch.uint8)
        gt_lpips_features[i * args.batch_size : (i + 1) * args.batch_size] = feats

    return gt_lpips_features


def concat_lpips_feats(lpips_metric, device, x):
    return torch.concatenate(
        [
            x_i.cpu().flatten(1)
            for x_i in lpips_metric.get_features(x["images"].to(device))
        ],
        dim=1,
    )


def correct_for_black_images(array_to_correct, non_black_imgs, default_fn):
    default_value_idcs = default_fn(non_black_imgs.shape, dtype=array_to_correct.dtype)
    default_value_idcs[non_black_imgs] = array_to_correct
    array_to_correct = default_value_idcs


# closest_recon_to_gt_ssim = correct_for_black_images(
#     closest_recon_to_gt_ssim,
#     non_black_imgs,
#     lambda *args, **kwargs: -torch.ones(*args, **kwargs),
# )
# min_dist_ssim = correct_for_black_images(
#     min_dist_ssim,
#     non_black_imgs,
#     lambda *args, **kwargs: torch.full(
#         *args, **kwargs, fill_value=torch.inf
#     ),
# )


def save_full_distance_matrix(
    save_dir: Path,
    batch_num: int,
    privacy_level: str,
    metric_name: str,
    distances: torch.Tensor,
    true_idcs,
):
    distances = pd.DataFrame(distances.numpy().T, index=true_idcs)
    save_dir = save_dir / "complete_distance_matrices"
    save_dir.mkdir(exist_ok=True, parents=True)
    distances.to_csv(str(save_dir / f"{metric_name}_{privacy_level}_{batch_num}.csv"))


if __name__ == "__main__":
    id_str = f"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
    parser = ArgumentParser()
    parser.add_argument(
        "--recon_folder",
        type=Path,
        default=None,
        required=True,
        help="Folder with reconstructions",
    )
    parser.add_argument(
        "--gpu_memory_limit",
        type=float,
        default=1e10,
        help="Threshold to limit the maximum gpu capacity. If lower will reduce memory consumption but take longer.",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1024,
        help="Batch size for calculating features",
    )
    parser.add_argument(
        "--num_workers", type=int, default=2, help="Number of workers for dataloader"
    )
    parser.add_argument(
        "--prefetch_factor", type=int, default=4, help="Prefetch factor for dataloader"
    )
    for metric in AVAILABLE_METRICS:
        parser.add_argument(
            f"--use_{metric}",
            action="store_true",
            help=f"Calculate {metric} as metric between images and reconstructions",
        )
        parser.add_argument(
            f"--{metric}_patchsize",
            type=int,
            default=1,
            help="For iterative calculation use this as patchsize to limit memory consumption",
        )
    parser.add_argument(
        "--lpips_layer",
        type=str,
        choices=["full", "first", "second", "intermediate"],
        default="full",
        help="Use features from certain layers for calculating perceptual distances",
    )
    parser.add_argument(
        "--threshold_black_imgs",
        type=float,
        default=-1,
        help="Constraint to only consider images that have more "
        "than this threshold of non-black pixels. "
        "Range is in [0,1]",
    )
    args = parser.parse_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16

    read_directory = Path(args.recon_folder)
    save_dir = read_directory / f"evaluation_{id_str}"
    privacy_level_dirs = {
        d.stem.replace("eps=", ""): d
        for d in read_directory.iterdir()
        if d.is_dir() and not "evaluation" in d.stem
    }
    privacy_level_dirs = dict(sorted(privacy_level_dirs.items()))
    gt_folder = privacy_level_dirs["gt"]
    assert gt_folder.is_dir()
    del privacy_level_dirs["gt"]
    batches = [int(d.stem) for d in gt_folder.iterdir() if d.is_dir()]
    batches = max(batches) + 1

    if args.use_fid:
        fid_metric = piq.FID()
        fid_extractor = piq.feature_extractors.InceptionV3()
    if args.use_lpips:
        if args.lpips_layer == "full":
            lpips_metric = piq.LPIPS()
        else:
            layers, weights = None, None
            match args.lpips_layer:
                case "first":
                    layers = ("relu1_2",)
                    weights = (1.0,)
                case "second":
                    layers = ("relu2_2",)
                    weights = (1.0,)
                case "intermediate":
                    layers = ("relu2_2", "relu3_3", "relu4_3")
                    weights = (1.0, 1.0, 1.0)
                case _:
                    raise ValueError(f"LPIPS layers {args.lpips_layer} not supported")
            lpips_metric = piq.ContentLoss(
                layers=layers,
                weights=weights,
                normalize_features=True,
            )
        lpips_metric.to(device)

        example_lpips_feat = concat_lpips_feats(
            lpips_metric,
            device,
            {
                "images": FolderDataset(
                    gt_folder / "0",
                    transform=ToTensor(),
                )[
                    0
                ].unsqueeze(0)
            },
        )
        lpips_feat_shape = example_lpips_feat.shape[1]

    results_dict = {
        "batch_id": [],
        "img_id": [],
        "gt_image": [],
    }
    for metric in AVAILABLE_METRICS:
        if getattr(args, f"use_{metric}"):
            results_dict.update(
                {
                    **{
                        f"closest_recon_{metric}_{privacy_level}": []
                        for privacy_level in privacy_level_dirs.keys()
                    },
                    **{
                        f"{metric}_distance_{privacy_level}": []
                        for privacy_level in privacy_level_dirs.keys()
                    },
                }
            )
    for batch_num in trange(batches, desc="Batches", leave=False):
        gt_batch_path = gt_folder / str(batch_num)
        gt_dataset = FolderDataset(
            gt_batch_path,
            transform=ToTensor(),
        )
        gt_loader = DataLoader(
            gt_dataset,
            collate_fn=collate_fn,
            num_workers=args.num_workers,
            batch_size=args.batch_size,
            pin_memory=False,
            prefetch_factor=args.prefetch_factor,
            shuffle=False,
        )
        if args.use_mse or args.use_ssim or args.threshold_black_imgs > 0.0:
            gt_images = torch.concat([batch["images"] for batch in gt_loader])
        imgs_in_use = torch.arange(non_black_imgs.shape[0])
        if args.threshold_black_imgs > 0.0:
            non_black_pxls = gt_images > 1.0 / 255.0
            non_black_imgs = (
                non_black_pxls.sum(dim=list(range(1, len(non_black_pxls.shape))))
                > np.prod(non_black_pxls.shape[1:]) * args.threshold_black_imgs
            )
            gt_images = gt_images[non_black_imgs]
            imgs_in_use = imgs_in_use[non_black_imgs]
            gt_dataset = FolderDataset(
                gt_batch_path,
                transform=ToTensor(),
                ignore_img_idcs=non_black_imgs.tolist(),
            )
            gt_loader = DataLoader(
                gt_dataset,
                collate_fn=collate_fn,
                num_workers=args.num_workers,
                batch_size=args.batch_size,
                pin_memory=False,
                prefetch_factor=args.prefetch_factor,
                shuffle=False,
            )
            del non_black_pxls
        imgs_in_use = imgs_in_use.tolist()
        with torch.inference_mode():  # , torch.autocast(device_type=device, dtype=dtype):
            if args.use_fid:
                gt_fid_features = fid_metric.compute_feats(
                    gt_loader, feature_extractor=fid_extractor
                )
            if args.use_lpips:
                with torch.autocast(device_type=device, dtype=dtype):
                    lpips_metric.to(device)
                    gt_lpips_features = calc_lpips_features(
                        args,
                        lpips_metric,
                        lpips_feat_shape,
                        device,
                        gt_loader,  # , dtype=dtype
                    )

        batch_dict = {
            "batch_id": [batch_num for _ in imgs_in_use],
            "img_id": imgs_in_use,
            "gt_image": [gt_batch_path / f"{i}.png" for i in imgs_in_use],
        }

        for privacy_level, recon_folder in tqdm(
            privacy_level_dirs.items(),
            total=len(privacy_level_dirs),
            desc="Privacy Levels",
            leave=False,
        ):
            recon_batch_path = recon_folder / str(batch_num)
            recon_loader = DataLoader(
                FolderDataset(
                    recon_batch_path,
                    transform=ToTensor(),
                ),
                collate_fn=collate_fn,
                num_workers=args.num_workers,
                batch_size=args.batch_size,
                pin_memory=False,
                prefetch_factor=args.prefetch_factor,
                shuffle=False,
            )

            with torch.inference_mode():  # , :
                if args.use_fid:
                    recon_fid_features = fid_metric.compute_feats(
                        recon_loader,
                        feature_extractor=fid_extractor,
                    )

                    torch.cuda.empty_cache()
                    min_dist_fid, closest_recon_to_gt_fid, full_matrix = calc_mse_diff(
                        args,
                        gt_fid_features,
                        recon_fid_features,
                        patchsize=args.fid_patchsize,
                    )
                    torch.cuda.empty_cache()
                    save_full_distance_matrix(
                        save_dir,
                        batch_num,
                        privacy_level,
                        f"fid",
                        full_matrix,
                        imgs_in_use,
                    )
                if args.use_lpips:
                    lpips_metric.to(device)
                    with torch.autocast(device_type=device, dtype=dtype):
                        recon_lpips_features = calc_lpips_features(
                            args,
                            lpips_metric,
                            lpips_feat_shape,
                            device,
                            recon_loader,
                            # dtype=dtype,
                        )
                    # assert np.isnan(recon_lpips_features).sum() == 0

                    lpips_metric.to("cpu")
                    torch.cuda.empty_cache()
                    (
                        min_dist_lpips,
                        closest_recon_to_gt_lpips,
                        full_matrix,
                    ) = calc_mse_diff(
                        args,
                        gt_lpips_features,
                        recon_lpips_features,
                        patchsize=args.lpips_patchsize,
                    )
                    save_full_distance_matrix(
                        save_dir,
                        batch_num,
                        privacy_level,
                        f"lpips_{args.lpips_layer}",
                        full_matrix,
                        imgs_in_use,
                    )
                    torch.cuda.empty_cache()

                if args.use_ssim or args.use_mse:
                    gt_images = gt_images.to(device)
                    recon_images = torch.concat(
                        [batch["images"] for batch in recon_loader]
                    ).to(device)
                if args.use_ssim:
                    ssim_dist = 1.0 - ssim_fn(
                        gt_images,
                        recon_images,
                        patchsize=args.ssim_patchsize,
                    )
                    # ssims = torch.randn(gt_images.shape[0], len(recon_loader.dataset))
                    closest_recon_to_gt_ssim = ssim_dist.argmin(dim=1)
                    min_dist_ssim = ssim_dist.min(dim=1)[0]
                    save_full_distance_matrix(
                        save_dir,
                        batch_num,
                        privacy_level,
                        "ssim",
                        ssim_dist,
                        imgs_in_use,
                    )
                if args.use_mse:
                    min_dist_mse, closest_recon_to_gt_mse, full_matrix = calc_mse_diff(
                        args,
                        gt_images.flatten(1),
                        recon_images.flatten(1),
                        patchsize=args.mse_patchsize,
                    )
                    save_full_distance_matrix(
                        save_dir,
                        batch_num,
                        privacy_level,
                        "mse",
                        full_matrix,
                        imgs_in_use,
                    )

                if args.use_ssim or args.use_mse:
                    gt_images = gt_images.cpu()
                    del recon_images
                    torch.cuda.empty_cache()

            if args.use_fid:
                batch_dict[
                    f"closest_recon_fid_{privacy_level}"
                ] = closest_recon_to_gt_fid.flatten().tolist()
                batch_dict[
                    f"fid_distance_{privacy_level}"
                ] = min_dist_fid.flatten().tolist()
            if args.use_lpips:
                batch_dict[
                    f"closest_recon_lpips_{args.lpips_layer}_{privacy_level}"
                ] = closest_recon_to_gt_lpips.flatten().tolist()
                batch_dict[
                    f"lpips_{args.lpips_layer}_distance_{privacy_level}"
                ] = min_dist_lpips.flatten().tolist()
            if args.use_ssim:
                batch_dict[
                    f"closest_recon_ssim_{privacy_level}"
                ] = closest_recon_to_gt_ssim.flatten().tolist()
                batch_dict[
                    f"ssim_distance_{privacy_level}"
                ] = min_dist_ssim.flatten().tolist()
            if args.use_mse:
                batch_dict[
                    f"closest_recon_mse_{privacy_level}"
                ] = closest_recon_to_gt_mse.flatten().tolist()
                batch_dict[
                    f"mse_distance_{privacy_level}"
                ] = min_dist_mse.flatten().tolist()
        for key, value in results_dict.items():
            results_dict[key] += batch_dict[key]

    total_df = pd.DataFrame.from_dict(results_dict)
    total_df.to_csv(str(save_dir / f"reconstruction_distances.csv"))
