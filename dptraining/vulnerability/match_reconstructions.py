from argparse import ArgumentParser
from pathlib import Path
from typing import Callable
from sys import path

import numpy as np
import pandas as pd
import piq
import torch
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import ToTensor
from tqdm import tqdm, trange
from warnings import warn, catch_warnings, simplefilter
from datetime import datetime
from PIL import Image


path.insert(0, str(Path.cwd()))

from dptraining.vulnerability.vectorized_ssim import (
    vectorized_all_vs_all_2d_ssim,
    simple_3d_slicewise_distances,
)


AVAILABLE_METRICS = [
    "mse",
    "ssim",
    "fid",
    "lpips",
]


def load_images_with_correct_number_of_channels(path: str):
    with open(path, "rb") as f:
        img = Image.open(f)
        img.load()
    return img


def calc_perc_dist_direct(features_gt, features_rec):
    if not torch.is_floating_point(features_gt):
        features_gt = features_gt.to(torch.float16, device=features_gt.device)
        features_rec = features_rec.to(torch.float16, device=features_rec.device)
    if features_gt.dtype == torch.float32:
        with catch_warnings(), torch.inference_mode():
            simplefilter("ignore")
            return torch.nn.functional.mse_loss(
                features_gt.view(features_gt.shape[0], 1, *features_gt.shape[1:]),
                features_rec,
                reduction="none",
            )[..., 0]

    return (
        torch.pow(
            features_gt.view(features_gt.shape[0], 1, *features_gt.shape[1:])
            - features_rec,
            2,
        )
        .sum(dim=2)
        .cpu()
    )


def mean_diff_torch_alongX(a1: torch.Tensor, a2: torch.Tensor):
    X, N = a1.shape
    X1, M = a2.shape
    assert X == X1
    out = torch.zeros((N, M), device=a1.device)
    for l, r in tqdm(zip(a1, a2), total=X, leave=False):
        out += torch.square(l[:, None] - r)
    out /= X
    return out


def mean_diff_torch_alongNX(
    a1: torch.Tensor,
    a2: torch.Tensor,
    patchsize: int = 50,
    iterative: bool = False,
    device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
):
    # device = a1.device
    N, X = a1.shape
    M, X1 = a2.shape
    assert X == X1
    out = torch.zeros((N, M), dtype=torch.float32)
    a1 = a1.T.clone()  # .to(dtype=torch.float16)
    if iterative:
        a2 = a2.T.clone().to(device)  # , dtype=torch.float16)
    else:
        # a1 = a1
        a2 = a2.clone().to(device)  # , dtype=torch.float16)
    N_patches = N // patchsize + (N % patchsize > 0) * 1
    for i in trange(N_patches, leave=False, desc="Distance patches"):
        if iterative:
            patch = a1[:, i * patchsize : (i + 1) * patchsize].to(device)  # .clone()
            out[i * patchsize : (i + 1) * patchsize] = mean_diff_torch_alongX(
                patch, a2
            ).cpu()
        else:
            torch.cuda.empty_cache()
            patch = a1[:, i * patchsize : (i + 1) * patchsize].T.clone().to(device)
            out[i * patchsize : (i + 1) * patchsize] = calc_perc_dist_direct(
                patch, a2
            ).cpu()
        del patch
    return out


def calc_mse_diff(
    args,
    features_gt,
    features_rec,
    device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    patchsize=1,
):
    N, M, X = features_gt.shape[0], features_rec.shape[0], features_gt.shape[1]
    imgs_per_iter: int = int(args.gpu_memory_limit / np.prod(features_rec.shape))
    if (
        N * M * X < args.gpu_memory_limit
    ):  # this is just an empirical threshold and strongly depends on the hardware
        perc_dist = calc_perc_dist_direct(
            features_gt.to(device), features_rec.to(device)
        )
    elif imgs_per_iter < 1:
        perc_dist = mean_diff_torch_alongNX(
            features_rec,
            features_gt,
            patchsize,
            iterative=True,
        ).T
    else:  # memory optimised version
        perc_dist = mean_diff_torch_alongNX(
            features_rec,
            features_gt,
            patchsize,
            iterative=False,
        ).T

    min_dist, closest_recon_to_gt = torch.min(perc_dist, dim=1)[0], torch.argmin(
        perc_dist, dim=1
    )
    return min_dist, closest_recon_to_gt, perc_dist


class FolderDataset(Dataset):
    def __init__(
        self,
        folder_path: Path,
        loader_fn: Callable = load_images_with_correct_number_of_channels,
        transform: Callable = torch.nn.Identity(),
        idcs_to_use: list[bool] = [],
        file_extensions: list[str] = [".png"],
    ) -> None:
        super().__init__()
        self.files = [
            f
            for f in folder_path.iterdir()
            if f.is_file() and f.suffix in file_extensions
        ]
        self.files = sorted(self.files, key=lambda x: int(x.stem))
        self.files = np.array(self.files)
        if len(idcs_to_use) > 0:
            self.files = self.files[idcs_to_use]
        self.loader_fn = loader_fn
        self.transform = transform

    def __len__(self) -> int:
        return len(self.files)

    def __getitem__(self, index) -> torch.Tensor:
        return self.transform(self.loader_fn(self.files[index]))


def collate_fn2d(batch_list: list[torch.Tensor]) -> dict[str, torch.Tensor]:
    return {"images": torch.stack(batch_list, dim=0)}


def collate_fn3d(batch_list: list[torch.Tensor]) -> dict[str, torch.Tensor]:
    return {"images": torch.concatenate(batch_list, dim=0)}


def calc_lpips_features(
    args, lpips_metric, lpips_feat_shape, device, dataloader, slices_per_img: int = 1
):
    lpips_features = torch.zeros(
        (len(dataloader.dataset), slices_per_img, lpips_feat_shape),
        dtype=torch.float16,
    )
    for i, x in tqdm(
        enumerate(dataloader),
        total=len(dataloader),
        desc="Calc lpips feats",
        leave=False,
    ):
        x = x["images"]
        feats = concat_lpips_feats(lpips_metric, device, x.view(-1, *x.shape[-3:]))
        lpips_features[i * args.batch_size : (i + 1) * args.batch_size] = feats.reshape(
            *x.shape[:2], -1
        )

    return lpips_features.squeeze(1)


def concat_lpips_feats(lpips_metric, device, x):
    return torch.concatenate(
        [x_i.cpu().flatten(1) for x_i in lpips_metric.get_features(x.to(device))],
        dim=1,
    )


def save_full_distance_matrix(
    save_dir: Path,
    batch_num: int,
    privacy_level: str,
    metric_name: str,
    distances: torch.Tensor,
    true_idcs,
):
    distances = pd.DataFrame(distances.detach().cpu().numpy(), index=true_idcs)
    save_dir = save_dir / "complete_distance_matrices"
    save_dir.mkdir(exist_ok=True, parents=True)
    distances.to_csv(str(save_dir / f"{metric_name}_{privacy_level}_{batch_num}.csv"))


def multi_dim_distance(gt_dataset, recon_dataset, full_matrix):
    N_layers = full_matrix.shape[0] / len(gt_dataset)
    assert N_layers == full_matrix.shape[1] / len(recon_dataset)
    N_layers = int(N_layers)
    full_matrix = full_matrix.reshape(
        len(gt_dataset), N_layers, len(recon_dataset), N_layers
    )
    full_matrix = full_matrix.sum(dim=(1, 3))
    min_dist_fid, closest_recon_to_gt_fid = (
        full_matrix.min(dim=1),
        full_matrix.argmin(dim=1)[0],
    )

    return min_dist_fid, closest_recon_to_gt_fid, full_matrix


def dist_matrix_to_results(distance_matrix):
    closest_recon_to_gt = distance_matrix.argmin(dim=1)
    min_dist = distance_matrix.min(dim=1)[0]
    return min_dist, closest_recon_to_gt


if __name__ == "__main__":
    id_str = f"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
    parser = ArgumentParser()
    parser.add_argument(
        "--recon_folder",
        type=Path,
        default=None,
        required=True,
        help="Folder with reconstructions",
    )
    parser.add_argument(
        "--gpu_memory_limit",
        type=float,
        default=1e10,
        help="Threshold to limit the maximum gpu capacity. If lower will reduce memory consumption but take longer.",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1024,
        help="Batch size for calculating features",
    )
    parser.add_argument(
        "--num_workers", type=int, default=2, help="Number of workers for dataloader"
    )
    parser.add_argument(
        "--prefetch_factor", type=int, default=4, help="Prefetch factor for dataloader"
    )
    for metric in AVAILABLE_METRICS:
        parser.add_argument(
            f"--use_{metric}",
            action="store_true",
            help=f"Calculate {metric} as metric between images and reconstructions",
        )
        parser.add_argument(
            f"--{metric}_patchsize",
            type=int,
            default=1,
            help="For iterative calculation use this as patchsize to limit memory consumption",
        )
    parser.add_argument(
        "--lpips_layer",
        type=str,
        choices=["full", "first", "second", "intermediate"],
        default="full",
        help="Use features from certain layers for calculating perceptual distances",
    )
    parser.add_argument(
        "--threshold_zero_pixels",
        type=float,
        default=-1,
        help="Constraint to only consider images that have more "
        "than this threshold of non-zero pixels. "
        "Range is in [0,1]. "
        "It is strongly recommended to only use this to limit computation time. "
        "Otherwise it is preferable to filter at later stages.",
    )
    parser.add_argument(
        "--multi_dim_input",
        action="store_true",
        help="Ground truth and reconstructions are arrays with > 2 dimensions "
        "and saved in npy files. We assume all arrays have the same shape!",
    )
    args = parser.parse_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16

    read_directory = Path(args.recon_folder)
    save_dir = read_directory / f"evaluation_{id_str}"
    privacy_level_dirs = {
        d.stem.replace("eps=", ""): d
        for d in read_directory.iterdir()
        if d.is_dir() and not "evaluation" in d.stem
    }
    privacy_level_dirs = dict(sorted(privacy_level_dirs.items()))
    gt_folder = privacy_level_dirs["gt"]
    assert gt_folder.is_dir()
    del privacy_level_dirs["gt"]
    batches = [int(d.stem) for d in gt_folder.iterdir() if d.is_dir()]
    batches = max(batches) + 1

    if args.multi_dim_input:
        if args.use_fid:
            raise ValueError("FID not compatible with 3d data yet")
        tf = lambda x: torch.from_numpy(
            x.transpose(-1, *tuple(range(len(x.shape) - 1)))
        ).to(dtype=torch.float32)
        file_extension = "npy"
        loader_fn = lambda x: np.load(x, allow_pickle=True, fix_imports=False)
        collate_fn = collate_fn2d
    else:
        tf = ToTensor()
        file_extension = "png"
        loader_fn = load_images_with_correct_number_of_channels
        collate_fn = collate_fn2d
    file_extensions = [f".{file_extension}"]

    if args.use_fid:
        fid_metric = piq.FID()
        fid_extractor = piq.feature_extractors.InceptionV3()
    if args.use_lpips:
        if args.lpips_layer == "full":
            lpips_metric = piq.LPIPS()
        else:
            layers, weights = None, None
            match args.lpips_layer:
                case "first":
                    layers = ("relu1_2",)
                    weights = (1.0,)
                case "second":
                    layers = ("relu2_2",)
                    weights = (1.0,)
                case "intermediate":
                    layers = ("relu2_2", "relu3_3", "relu4_3")
                    weights = (1.0, 1.0, 1.0)
                case _:
                    raise ValueError(f"LPIPS layers {args.lpips_layer} not supported")
            lpips_metric = piq.ContentLoss(
                layers=layers,
                weights=weights,
                normalize_features=True,
            )
        lpips_metric.to(device)

        example_batch = FolderDataset(
            gt_folder / "0",
            transform=tf,
            file_extensions=file_extensions,
            loader_fn=loader_fn,
        )[0]
        if not args.multi_dim_input:
            example_batch.unsqueeze(0)
        example_lpips_feat = concat_lpips_feats(
            lpips_metric,
            device,
            example_batch,
        )
        lpips_feat_shape = example_lpips_feat.shape[1]
        del example_batch, example_lpips_feat

    results_dict = {
        "batch_id": [],
        "img_id": [],
        "gt_image": [],
        "non_zero_ratio": [],
    }
    for metric in AVAILABLE_METRICS:
        if getattr(args, f"use_{metric}"):
            results_dict.update(
                {
                    **{
                        f"closest_recon_{metric}_{privacy_level}": []
                        for privacy_level in privacy_level_dirs.keys()
                    },
                    **{
                        f"{metric}_distance_{privacy_level}": []
                        for privacy_level in privacy_level_dirs.keys()
                    },
                }
            )
    for batch_num in trange(batches, desc="Batches", leave=False):
        gt_batch_path = gt_folder / str(batch_num)
        gt_dataset = FolderDataset(
            gt_batch_path,
            transform=tf,
            file_extensions=file_extensions,
            loader_fn=loader_fn,
        )
        N_slices = gt_dataset[0].shape[0]
        gt_loader = DataLoader(
            gt_dataset,
            collate_fn=collate_fn,
            num_workers=args.num_workers,
            batch_size=args.batch_size,
            pin_memory=False,
            prefetch_factor=args.prefetch_factor,
            shuffle=False,
        )
        gt_images = torch.concat([batch["images"] for batch in gt_loader])
        imgs_in_use = torch.arange(len(gt_dataset))
        non_zero_pxls = gt_images > 1.0 / 255.0
        non_zero_percentage_per_image = non_zero_pxls.sum(
            dim=list(range(1, len(non_zero_pxls.shape)))
        ) / np.prod(non_zero_pxls.shape[1:])
        if args.threshold_zero_pixels > 0.0:
            non_zero_imgs = non_zero_percentage_per_image > args.threshold_zero_pixels
            gt_images = gt_images[non_zero_imgs]
            imgs_in_use = imgs_in_use[non_zero_imgs]
            non_zero_percentage_per_image[non_zero_imgs]
            if torch.any(~non_zero_imgs).item():
                gt_dataset = FolderDataset(
                    gt_batch_path,
                    transform=tf,
                    file_extensions=file_extensions,
                    loader_fn=loader_fn,
                    idcs_to_use=imgs_in_use,
                )
                gt_loader = DataLoader(
                    gt_dataset,
                    collate_fn=collate_fn,
                    num_workers=args.num_workers,
                    batch_size=args.batch_size,
                    pin_memory=False,
                    prefetch_factor=args.prefetch_factor,
                    shuffle=False,
                )
            del non_zero_pxls
        imgs_in_use = imgs_in_use.tolist()
        with torch.inference_mode():  # , torch.autocast(device_type=device, dtype=dtype):
            if args.use_fid:
                gt_fid_features = fid_metric.compute_feats(
                    gt_loader, feature_extractor=fid_extractor
                )
            if args.use_lpips:
                with torch.autocast(device_type=device, dtype=dtype):
                    lpips_metric.to(device)
                    gt_lpips_features = calc_lpips_features(
                        args,
                        lpips_metric,
                        lpips_feat_shape,
                        device,
                        gt_loader,
                        slices_per_img=N_slices,
                    )

        batch_dict = {
            "batch_id": [batch_num for _ in imgs_in_use],
            "img_id": imgs_in_use,
            "gt_image": [gt_batch_path / f"{i}.{file_extension}" for i in imgs_in_use],
            "non_zero_ratio": non_zero_percentage_per_image.tolist(),
        }

        for privacy_level, recon_folder in tqdm(
            privacy_level_dirs.items(),
            total=len(privacy_level_dirs),
            desc="Privacy Levels",
            leave=False,
        ):
            recon_batch_path = recon_folder / str(batch_num)
            recon_dataset = FolderDataset(
                recon_batch_path,
                transform=tf,
                file_extensions=file_extensions,
                loader_fn=loader_fn,
            )
            recon_loader = DataLoader(
                recon_dataset,
                collate_fn=collate_fn,
                num_workers=args.num_workers,
                batch_size=args.batch_size,
                pin_memory=False,
                prefetch_factor=args.prefetch_factor,
                shuffle=False,
            )

            with torch.inference_mode():  # , :
                if args.use_fid:
                    recon_fid_features = fid_metric.compute_feats(
                        recon_loader,
                        feature_extractor=fid_extractor,
                    )

                    torch.cuda.empty_cache()
                    min_dist_fid, closest_recon_to_gt_fid, full_matrix = calc_mse_diff(
                        args,
                        gt_fid_features,
                        recon_fid_features,
                        patchsize=args.fid_patchsize,
                    )
                    torch.cuda.empty_cache()
                    if args.multi_dim_input:
                        (
                            min_dist_fid,
                            closest_recon_to_gt_fid,
                            full_matrix,
                        ) = multi_dim_distance(gt_dataset, recon_dataset, full_matrix)
                    save_full_distance_matrix(
                        save_dir,
                        batch_num,
                        privacy_level,
                        f"fid",
                        full_matrix,
                        imgs_in_use,
                    )
                if args.use_lpips:
                    lpips_metric.to(device)
                    with torch.autocast(device_type=device, dtype=dtype):
                        recon_lpips_features = calc_lpips_features(
                            args,
                            lpips_metric,
                            lpips_feat_shape,
                            device,
                            recon_loader,
                            slices_per_img=N_slices,
                        )
                    # assert np.isnan(recon_lpips_features).sum() == 0

                    lpips_metric.to("cpu")
                    torch.cuda.empty_cache()
                    if args.multi_dim_input:
                        full_matrix = simple_3d_slicewise_distances(
                            gt_lpips_features.to(torch.float32),
                            recon_lpips_features.to(torch.float32),
                            torch.nn.functional.mse_loss,
                        )
                        (
                            min_dist_lpips,
                            closest_recon_to_gt_lpips,
                        ) = dist_matrix_to_results(full_matrix)
                    else:
                        (
                            min_dist_lpips,
                            closest_recon_to_gt_lpips,
                            full_matrix,
                        ) = calc_mse_diff(
                            args,
                            gt_lpips_features,
                            recon_lpips_features,
                            patchsize=args.lpips_patchsize,
                        )
                    save_full_distance_matrix(
                        save_dir,
                        batch_num,
                        privacy_level,
                        f"lpips_{args.lpips_layer}",
                        full_matrix,
                        imgs_in_use,
                    )
                    torch.cuda.empty_cache()

                if args.use_ssim or args.use_mse:
                    gt_images = gt_images.to(device)
                    recon_images = torch.concat(
                        [batch["images"] for batch in recon_loader]
                    ).to(device)
                if args.use_ssim:
                    if args.multi_dim_input:
                        ssim_dist = 1.0 - simple_3d_slicewise_distances(
                            gt_images, recon_images, dist_fn=piq.ssim
                        )
                    else:
                        ssim_dist = 1.0 - vectorized_all_vs_all_2d_ssim(
                            gt_images,
                            recon_images,
                            patchsize=args.ssim_patchsize,
                        )
                    # ssims = torch.randn(gt_images.shape[0], len(recon_loader.dataset))
                    min_dist_ssim, closest_recon_to_gt_ssim = dist_matrix_to_results(
                        ssim_dist
                    )
                    save_full_distance_matrix(
                        save_dir,
                        batch_num,
                        privacy_level,
                        "ssim",
                        ssim_dist,
                        imgs_in_use,
                    )
                if args.use_mse:
                    min_dist_mse, closest_recon_to_gt_mse, full_matrix = calc_mse_diff(
                        args,
                        gt_images.flatten(1),
                        recon_images.flatten(1),
                        patchsize=args.mse_patchsize,
                    )
                    save_full_distance_matrix(
                        save_dir,
                        batch_num,
                        privacy_level,
                        "mse",
                        full_matrix,
                        imgs_in_use,
                    )

                if args.use_ssim or args.use_mse:
                    gt_images = gt_images.cpu()
                    del recon_images
                    torch.cuda.empty_cache()

            if args.use_fid:
                batch_dict[
                    f"closest_recon_fid_{privacy_level}"
                ] = closest_recon_to_gt_fid.flatten().tolist()
                batch_dict[
                    f"fid_distance_{privacy_level}"
                ] = min_dist_fid.flatten().tolist()
            if args.use_lpips:
                batch_dict[
                    f"closest_recon_lpips_{privacy_level}"
                ] = closest_recon_to_gt_lpips.flatten().tolist()
                batch_dict[
                    f"lpips_distance_{privacy_level}"
                ] = min_dist_lpips.flatten().tolist()
            if args.use_ssim:
                batch_dict[
                    f"closest_recon_ssim_{privacy_level}"
                ] = closest_recon_to_gt_ssim.flatten().tolist()
                batch_dict[
                    f"ssim_distance_{privacy_level}"
                ] = min_dist_ssim.flatten().tolist()
            if args.use_mse:
                batch_dict[
                    f"closest_recon_mse_{privacy_level}"
                ] = closest_recon_to_gt_mse.flatten().tolist()
                batch_dict[
                    f"mse_distance_{privacy_level}"
                ] = min_dist_mse.flatten().tolist()
        for key, value in results_dict.items():
            results_dict[key] += batch_dict[key]

    total_df = pd.DataFrame.from_dict(results_dict)
    total_df.to_csv(str(save_dir / f"reconstruction_distances.csv"))
